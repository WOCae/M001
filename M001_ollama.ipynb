{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "83211fab",
      "metadata": {
        "id": "83211fab"
      },
      "source": [
        "ollamaã®èµ·å‹•"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f1f81f4",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q pypdf\n",
        "!pip install -q pypdf nbformat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43b7099a",
      "metadata": {
        "id": "43b7099a"
      },
      "outputs": [],
      "source": [
        "# 1. ä¾å­˜ãƒ„ãƒ¼ãƒ«ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "!apt-get update && apt-get install -y zstd pciutils\n",
        "\n",
        "# 2. Ollamaã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "\n",
        "# å…±é€šè¨­å®šï¼ˆã“ã“ã§ãƒ¢ãƒ‡ãƒ«åã‚’ä¸€æ‹¬ç®¡ç†ã—ã¾ã™ï¼‰\n",
        "# ä½¿ç”¨ã—ãŸã„ãƒ¢ãƒ‡ãƒ«åï¼ˆglm4, llama3, qwen2-vl ãªã©ï¼‰ã‚’ã“ã“ã«è¨˜è¿°\n",
        "MODEL_NAME = \"qwen3-vl:4b\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6c8f6c1",
      "metadata": {
        "id": "c6c8f6c1"
      },
      "outputs": [],
      "source": [
        "def setup_ollama_server(model_name):\n",
        "    \"\"\"ã‚µãƒ¼ãƒãƒ¼ã‚’å†èµ·å‹•ã—ã€æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’æº–å‚™ã™ã‚‹\"\"\"\n",
        "    print(\"ğŸ”„ ã‚µãƒ¼ãƒãƒ¼ã‚’ãƒªã‚»ãƒƒãƒˆä¸­...\")\n",
        "    !pkill ollama\n",
        "    time.sleep(2)\n",
        "\n",
        "    # ã‚µãƒ¼ãƒãƒ¼ã‚’ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§èµ·å‹•\n",
        "    subprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "    time.sleep(5)\n",
        "\n",
        "    print(f\"â³ ãƒ¢ãƒ‡ãƒ« '{model_name}' ã‚’æº–å‚™ä¸­...\")\n",
        "    subprocess.run([\"ollama\", \"pull\", model_name])\n",
        "\n",
        "    # æ¥ç¶šç¢ºèª\n",
        "    try:\n",
        "        res = requests.get(\"http://localhost:11434/api/tags\")\n",
        "        if res.status_code == 200:\n",
        "            models = [m['name'] for m in res.json()['models']]\n",
        "            print(f\"âœ… Ollama server is live! åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«: {models}\")\n",
        "            return True\n",
        "    except:\n",
        "        print(\"âŒ ã‚µãƒ¼ãƒãƒ¼ã®èµ·å‹•ã«å¤±æ•—ã—ã¾ã—ãŸã€‚\")\n",
        "        return False\n",
        "\n",
        "# å®Ÿè¡Œ\n",
        "setup_ollama_server(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fhni7ivwGNsf",
      "metadata": {
        "id": "fhni7ivwGNsf"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import requests\n",
        "import json\n",
        "import base64\n",
        "import nbformat # Jupyter Notebookè§£æç”¨\n",
        "from pypdf import PdfReader\n",
        "\n",
        "def extract_text_from_file(file_path):\n",
        "    \"\"\"ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã«å¿œã˜ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹\"\"\"\n",
        "    ext = file_path.lower()\n",
        "    \n",
        "    # Jupyter Notebook (.ipynb) ã®å‡¦ç†\n",
        "    if ext.endswith('.ipynb'):\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                nb = nbformat.read(f, as_version=4)\n",
        "                content = []\n",
        "                for cell in nb.cells:\n",
        "                    if cell.cell_type in ['code', 'markdown']:\n",
        "                        content.append(f\"[{cell.cell_type}]\\n{cell.source}\")\n",
        "                return \"\\n\\n\".join(content)\n",
        "        except Exception as e:\n",
        "            return f\"Error reading notebook: {str(e)}\"\n",
        "\n",
        "    # PDFã®å‡¦ç†\n",
        "    elif ext.endswith('.pdf'):\n",
        "        try:\n",
        "            reader = PdfReader(file_path)\n",
        "            return \"\\n\".join([page.extract_text() for page in reader.pages if page.extract_text()])\n",
        "        except Exception as e:\n",
        "            return f\"Error reading PDF: {str(e)}\"\n",
        "\n",
        "    # ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹ã®ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆHTML, JS, CSS, Pythonãªã©ï¼‰ã‚’ä¸€æ‹¬å‡¦ç†\n",
        "    elif ext.endswith(('.txt', '.py', '.md', '.csv', '.html', '.js', '.css', '.json', '.ts')):\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                return f.read()\n",
        "        except Exception as e:\n",
        "            return f\"Error reading text file: {str(e)}\"\n",
        "            \n",
        "    return \"\"\n",
        "\n",
        "def encode_image_to_base64(image_path):\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
        "\n",
        "def chat_with_model(message, history):\n",
        "    user_text = message[\"text\"]\n",
        "    user_files = message[\"files\"]\n",
        "    \n",
        "    model = MODEL_NAME if 'MODEL_NAME' in globals() else \"qwen3-vl:4b\"\n",
        "    url = \"http://localhost:11434/api/chat\"\n",
        "\n",
        "    extracted_contexts = []\n",
        "    images_base64 = []\n",
        "\n",
        "    # è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ‹¬å‡¦ç†\n",
        "    for f in user_files:\n",
        "        if f.lower().endswith(('.png', '.jpg', '.jpeg', '.webp')):\n",
        "            images_base64.append(encode_image_to_base64(f))\n",
        "        else:\n",
        "            content = extract_text_from_file(f)\n",
        "            if content:\n",
        "                # ãƒ•ã‚¡ã‚¤ãƒ«åã¨ä¸­èº«ã‚’ã‚»ãƒƒãƒˆã«ã™ã‚‹\n",
        "                extracted_contexts.append(f\"--- File: {f} ---\\n{content}\")\n",
        "\n",
        "    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®çµ„ã¿ç«‹ã¦\n",
        "    full_prompt = user_text\n",
        "    if extracted_contexts:\n",
        "        context_str = \"\\n\\n\".join(extracted_contexts)\n",
        "        full_prompt = (\n",
        "            f\"ä»¥ä¸‹ã®æ·»ä»˜ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„:\\n\\n{context_str}\\n\\n\"\n",
        "            f\"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®æŒ‡ç¤º: {user_text}\"\n",
        "        )\n",
        "\n",
        "    current_msg = {\"role\": \"user\", \"content\": full_prompt}\n",
        "    if images_base64:\n",
        "        current_msg[\"images\"] = images_base64\n",
        "\n",
        "    # å±¥æ­´ã®å¤‰æ›ï¼ˆéå»ã®ã‚„ã‚Šå–ã‚Šã‚‚è€ƒæ…®ï¼‰\n",
        "    msgs = [{\"role\": \"user\", \"content\": h[0]} for h in history] + \\\n",
        "           [{\"role\": \"assistant\", \"content\": h[1]} for h in history]\n",
        "    msgs.append(current_msg)\n",
        "\n",
        "    try:\n",
        "        response = requests.post(url, json={\"model\": model, \"messages\": msgs, \"stream\": False}, timeout=180)\n",
        "        response.raise_for_status()\n",
        "        return response.json()['message']['content']\n",
        "    except Exception as e:\n",
        "        return f\"âš ï¸ ã‚¨ãƒ©ãƒ¼: {str(e)}\"\n",
        "\n",
        "# UIå´ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ•ã‚£ãƒ«ã‚¿ã‚‚æ›´æ–°\n",
        "demo = gr.ChatInterface(\n",
        "    fn=chat_with_model,\n",
        "    title=f\"ğŸ¤– Full-Stack File Explorer ({MODEL_NAME})\",\n",
        "    multimodal=True,\n",
        "    textbox=gr.MultimodalTextbox(file_types=[\"image\", \".txt\", \".pdf\", \".ipynb\", \".py\", \".html\", \".js\", \".css\"]),\n",
        "    theme=\"soft\"\n",
        ")\n",
        "\n",
        "demo.launch(share=True, debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40a303d6",
      "metadata": {
        "id": "40a303d6"
      },
      "outputs": [],
      "source": [
        "# def launch_vscode_tunnel():\n",
        "#     \"\"\"VS Code Tunnelã‚’èµ·å‹•ã—ã€GitHubèªè¨¼URLã‚’è¡¨ç¤ºã™ã‚‹\"\"\"\n",
        "#     import os\n",
        "\n",
        "#     # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã®ã¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
        "#     if not os.path.exists(\"code\"):\n",
        "#         print(\"â³ VS Code CLI ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...\")\n",
        "#         !curl -Lk 'https://code.visualstudio.com/sha/download?build=stable&os=cli-alpine-x64' --output vscode_cli.tar.gz\n",
        "#         !tar -xf vscode_cli.tar.gz\n",
        "\n",
        "#     print(\"\\n\" + \"=\"*50)\n",
        "#     print(\"ğŸš€ VS Code Tunnel ã‚’é–‹å§‹ã—ã¾ã™...\")\n",
        "#     print(\"â€» URLãŒè¡¨ç¤ºã•ã‚Œã‚‹ã¾ã§ 30ç§’ã€œ1åˆ†ã»ã©ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚\")\n",
        "#     print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "#     # èªè¨¼æƒ…å ±ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦ç¢ºå®Ÿã«æ–°ã—ã„ãƒªãƒ³ã‚¯ã‚’å‡ºã™\n",
        "#     # ä»¥å‰ã® `--provider` ã‚¨ãƒ©ãƒ¼ã‚’é¿ã‘ã‚‹ãŸã‚ã€æœ€å°é™ã®å¼•æ•°ã§å®Ÿè¡Œã—ã¾ã™\n",
        "#     !./code tunnel user logout\n",
        "#     !./code tunnel --accept-server-license-terms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_htIRXzRGC7R",
      "metadata": {
        "id": "_htIRXzRGC7R"
      },
      "outputs": [],
      "source": [
        "# !pkill code  # æ®‹ã£ã¦ã„ã‚‹VS Codeãƒ—ãƒ­ã‚»ã‚¹ã‚’å¼·åˆ¶çµ‚äº†"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.10.11)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
