{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "83211fab",
      "metadata": {
        "id": "83211fab"
      },
      "source": [
        "ollamaã®èµ·å‹•"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43b7099a",
      "metadata": {
        "id": "43b7099a"
      },
      "outputs": [],
      "source": [
        "# 1. ä¾å­˜ãƒ„ãƒ¼ãƒ«ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "!apt-get update && apt-get install -y zstd pciutils\n",
        "\n",
        "# 2. Ollamaã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "\n",
        "# å…±é€šè¨­å®šï¼ˆã“ã“ã§ãƒ¢ãƒ‡ãƒ«åã‚’ä¸€æ‹¬ç®¡ç†ã—ã¾ã™ï¼‰\n",
        "# ä½¿ç”¨ã—ãŸã„ãƒ¢ãƒ‡ãƒ«åï¼ˆglm4, llama3, qwen2-vl ãªã©ï¼‰ã‚’ã“ã“ã«è¨˜è¿°\n",
        "MODEL_NAME = \"qwen3-vl:4b\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6c8f6c1",
      "metadata": {
        "id": "c6c8f6c1"
      },
      "outputs": [],
      "source": [
        "def setup_ollama_server(model_name):\n",
        "    \"\"\"ã‚µãƒ¼ãƒãƒ¼ã‚’å†èµ·å‹•ã—ã€æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’æº–å‚™ã™ã‚‹\"\"\"\n",
        "    print(\"ğŸ”„ ã‚µãƒ¼ãƒãƒ¼ã‚’ãƒªã‚»ãƒƒãƒˆä¸­...\")\n",
        "    !pkill ollama\n",
        "    time.sleep(2)\n",
        "\n",
        "    # ã‚µãƒ¼ãƒãƒ¼ã‚’ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§èµ·å‹•\n",
        "    subprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "    time.sleep(5)\n",
        "\n",
        "    print(f\"â³ ãƒ¢ãƒ‡ãƒ« '{model_name}' ã‚’æº–å‚™ä¸­...\")\n",
        "    subprocess.run([\"ollama\", \"pull\", model_name])\n",
        "\n",
        "    # æ¥ç¶šç¢ºèª\n",
        "    try:\n",
        "        res = requests.get(\"http://localhost:11434/api/tags\")\n",
        "        if res.status_code == 200:\n",
        "            models = [m['name'] for m in res.json()['models']]\n",
        "            print(f\"âœ… Ollama server is live! åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«: {models}\")\n",
        "            return True\n",
        "    except:\n",
        "        print(\"âŒ ã‚µãƒ¼ãƒãƒ¼ã®èµ·å‹•ã«å¤±æ•—ã—ã¾ã—ãŸã€‚\")\n",
        "        return False\n",
        "\n",
        "# å®Ÿè¡Œ\n",
        "setup_ollama_server(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fhni7ivwGNsf",
      "metadata": {
        "id": "fhni7ivwGNsf"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import requests\n",
        "import json\n",
        "import base64\n",
        "\n",
        "def encode_image_to_base64(image_path):\n",
        "    \"\"\"ç”»åƒã‚’Base64å½¢å¼ã«å¤‰æ›ã™ã‚‹\"\"\"\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
        "\n",
        "def chat_with_model(message, history):\n",
        "    # message ã¯ {\"text\": \"...\", \"files\": [\"path/to/file\", ...]} ã¨ã„ã†è¾æ›¸å½¢å¼ã§æ¸¡ã•ã‚Œã¾ã™\n",
        "    user_text = message[\"text\"]\n",
        "    user_files = message[\"files\"]\n",
        "    \n",
        "    model = MODEL_NAME if 'MODEL_NAME' in globals() else \"qwen3-vl:4b\"\n",
        "    url = \"http://localhost:11434/api/chat\"\n",
        "\n",
        "    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æ§‹ç¯‰\n",
        "    current_msg = {\"role\": \"user\", \"content\": user_text}\n",
        "    \n",
        "    # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆã€Ollamaã®APIå½¢å¼ã«åˆã‚ã›ã¦è¿½åŠ \n",
        "    if user_files:\n",
        "        images_base64 = []\n",
        "        for f in user_files:\n",
        "            if f.lower().endswith(('.png', '.jpg', '.jpeg', '.webp')):\n",
        "                images_base64.append(encode_image_to_base64(f))\n",
        "        \n",
        "        if images_base64:\n",
        "            current_msg[\"images\"] = images_base64\n",
        "\n",
        "    # å±¥æ­´ã®å¤‰æ›ï¼ˆç°¡æ˜“åŒ–ã®ãŸã‚ã€éå»ã®ç”»åƒã¯å«ã‚ãªã„æ§‹æˆï¼‰\n",
        "    msgs = []\n",
        "    for h in history:\n",
        "        # historyå†…ã®æ§‹é€ ã«åˆã‚ã›ã¦èª¿æ•´ãŒå¿…è¦\n",
        "        msgs.append({\"role\": \"user\", \"content\": h[0]})\n",
        "        msgs.append({\"role\": \"assistant\", \"content\": h[1]})\n",
        "    \n",
        "    msgs.append(current_msg)\n",
        "\n",
        "    payload = {\n",
        "        \"model\": model,\n",
        "        \"messages\": msgs,\n",
        "        \"stream\": False\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.post(url, json=payload, timeout=120)\n",
        "        response.raise_for_status()\n",
        "        return response.json()['message']['content']\n",
        "    except Exception as e:\n",
        "        return f\"âš ï¸ ã‚¨ãƒ©ãƒ¼: {str(e)}\"\n",
        "\n",
        "# UIã®èµ·å‹•\n",
        "demo = gr.ChatInterface(\n",
        "    fn=chat_with_model,\n",
        "    title=f\"ğŸ¤– {MODEL_NAME} Multimodal Chat\",\n",
        "    multimodal=True,  # ã“ã‚Œã§ãƒ•ã‚¡ã‚¤ãƒ«æ·»ä»˜ãƒœã‚¿ãƒ³ãŒè¡¨ç¤ºã•ã‚Œã¾ã™\n",
        "    textbox=gr.MultimodalTextbox(file_types=[\"image\"]), # ç”»åƒã®ã¿ã«åˆ¶é™ã™ã‚‹å ´åˆ\n",
        "    theme=\"soft\"\n",
        ")\n",
        "\n",
        "demo.launch(share=True, debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40a303d6",
      "metadata": {
        "id": "40a303d6"
      },
      "outputs": [],
      "source": [
        "# def launch_vscode_tunnel():\n",
        "#     \"\"\"VS Code Tunnelã‚’èµ·å‹•ã—ã€GitHubèªè¨¼URLã‚’è¡¨ç¤ºã™ã‚‹\"\"\"\n",
        "#     import os\n",
        "\n",
        "#     # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã®ã¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
        "#     if not os.path.exists(\"code\"):\n",
        "#         print(\"â³ VS Code CLI ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...\")\n",
        "#         !curl -Lk 'https://code.visualstudio.com/sha/download?build=stable&os=cli-alpine-x64' --output vscode_cli.tar.gz\n",
        "#         !tar -xf vscode_cli.tar.gz\n",
        "\n",
        "#     print(\"\\n\" + \"=\"*50)\n",
        "#     print(\"ğŸš€ VS Code Tunnel ã‚’é–‹å§‹ã—ã¾ã™...\")\n",
        "#     print(\"â€» URLãŒè¡¨ç¤ºã•ã‚Œã‚‹ã¾ã§ 30ç§’ã€œ1åˆ†ã»ã©ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚\")\n",
        "#     print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "#     # èªè¨¼æƒ…å ±ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦ç¢ºå®Ÿã«æ–°ã—ã„ãƒªãƒ³ã‚¯ã‚’å‡ºã™\n",
        "#     # ä»¥å‰ã® `--provider` ã‚¨ãƒ©ãƒ¼ã‚’é¿ã‘ã‚‹ãŸã‚ã€æœ€å°é™ã®å¼•æ•°ã§å®Ÿè¡Œã—ã¾ã™\n",
        "#     !./code tunnel user logout\n",
        "#     !./code tunnel --accept-server-license-terms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_htIRXzRGC7R",
      "metadata": {
        "id": "_htIRXzRGC7R"
      },
      "outputs": [],
      "source": [
        "# !pkill code  # æ®‹ã£ã¦ã„ã‚‹VS Codeãƒ—ãƒ­ã‚»ã‚¹ã‚’å¼·åˆ¶çµ‚äº†"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
