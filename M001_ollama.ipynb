{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "83211fab",
      "metadata": {
        "id": "83211fab"
      },
      "source": [
        "ollamaã®èµ·å‹•"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43b7099a",
      "metadata": {
        "id": "43b7099a"
      },
      "outputs": [],
      "source": [
        "# 1. ä¾å­˜ãƒ„ãƒ¼ãƒ«ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "!apt-get update && apt-get install -y zstd pciutils\n",
        "\n",
        "# 2. Ollamaã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "\n",
        "# å…±é€šè¨­å®šï¼ˆã“ã“ã§ãƒ¢ãƒ‡ãƒ«åã‚’ä¸€æ‹¬ç®¡ç†ã—ã¾ã™ï¼‰\n",
        "# ä½¿ç”¨ã—ãŸã„ãƒ¢ãƒ‡ãƒ«åï¼ˆglm4, llama3, qwen2-vl ãªã©ï¼‰ã‚’ã“ã“ã«è¨˜è¿°\n",
        "MODEL_NAME = \"qwen3-vl:4b\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6c8f6c1",
      "metadata": {
        "id": "c6c8f6c1"
      },
      "outputs": [],
      "source": [
        "def setup_ollama_server(model_name):\n",
        "    \"\"\"ã‚µãƒ¼ãƒãƒ¼ã‚’å†èµ·å‹•ã—ã€æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’æº–å‚™ã™ã‚‹\"\"\"\n",
        "    print(\"ğŸ”„ ã‚µãƒ¼ãƒãƒ¼ã‚’ãƒªã‚»ãƒƒãƒˆä¸­...\")\n",
        "    !pkill ollama\n",
        "    time.sleep(2)\n",
        "\n",
        "    # ã‚µãƒ¼ãƒãƒ¼ã‚’ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§èµ·å‹•\n",
        "    subprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "    time.sleep(5)\n",
        "\n",
        "    print(f\"â³ ãƒ¢ãƒ‡ãƒ« '{model_name}' ã‚’æº–å‚™ä¸­...\")\n",
        "    subprocess.run([\"ollama\", \"pull\", model_name])\n",
        "\n",
        "    # æ¥ç¶šç¢ºèª\n",
        "    try:\n",
        "        res = requests.get(\"http://localhost:11434/api/tags\")\n",
        "        if res.status_code == 200:\n",
        "            models = [m['name'] for m in res.json()['models']]\n",
        "            print(f\"âœ… Ollama server is live! åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«: {models}\")\n",
        "            return True\n",
        "    except:\n",
        "        print(\"âŒ ã‚µãƒ¼ãƒãƒ¼ã®èµ·å‹•ã«å¤±æ•—ã—ã¾ã—ãŸã€‚\")\n",
        "        return False\n",
        "\n",
        "# å®Ÿè¡Œ\n",
        "setup_ollama_server(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fhni7ivwGNsf",
      "metadata": {
        "id": "fhni7ivwGNsf"
      },
      "outputs": [],
      "source": [
        "!pip install -q gradio\n",
        "\n",
        "import gradio as gr\n",
        "import requests\n",
        "import json\n",
        "\n",
        "def chat_with_model(message, history):\n",
        "    # ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å†…ã®è¨­å®š MODEL_NAME ã‚’å‚ç…§ï¼ˆæœªå®šç¾©ãªã‚‰ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’ä½¿ç”¨ï¼‰\n",
        "    model = MODEL_NAME if 'MODEL_NAME' in globals() else \"glm-4.7-flash\"\n",
        "    url = \"http://localhost:11434/api/chat\"\n",
        "\n",
        "    # Gradioã®å±¥æ­´ã‚’Ollamaã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å½¢å¼ã«å¤‰æ›\n",
        "    msgs = []\n",
        "    for user_msg, ai_msg in history:\n",
        "        msgs.append({\"role\": \"user\", \"content\": user_msg})\n",
        "        msgs.append({\"role\": \"assistant\", \"content\": ai_msg})\n",
        "    msgs.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    payload = {\n",
        "        \"model\": model,\n",
        "        \"messages\": msgs,\n",
        "        \"stream\": False\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        # ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰æ™‚é–“ã‚’è€ƒæ…®ã—ã€ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’é•·ã‚ã«è¨­å®š\n",
        "        response = requests.post(url, json=payload, timeout=120)\n",
        "        response.raise_for_status()\n",
        "        return response.json()['message']['content']\n",
        "    except Exception as e:\n",
        "        return f\"âš ï¸ ã‚¨ãƒ©ãƒ¼: {str(e)}\\nOllamaã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚\"\n",
        "\n",
        "# UIã®ãƒ‡ã‚¶ã‚¤ãƒ³è¨­å®š\n",
        "demo = gr.ChatInterface(\n",
        "    fn=chat_with_model,\n",
        "    title=f\"ğŸ¤– {MODEL_NAME if 'MODEL_NAME' in globals() else 'AI'} Chat UI\",\n",
        "    description=\"ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›ã—ã¦é€ä¿¡ã—ã¦ãã ã•ã„ã€‚å±¥æ­´ã‚‚è€ƒæ…®ã•ã‚Œã¾ã™ã€‚\",\n",
        "    theme=\"soft\"\n",
        ")\n",
        "\n",
        "# share=True ã«ã™ã‚‹ã¨å¤–éƒ¨URLï¼ˆgradio.liveï¼‰ãŒç™ºè¡Œã•ã‚Œã¾ã™\n",
        "demo.launch(share=True, debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40a303d6",
      "metadata": {
        "id": "40a303d6"
      },
      "outputs": [],
      "source": [
        "# def launch_vscode_tunnel():\n",
        "#     \"\"\"VS Code Tunnelã‚’èµ·å‹•ã—ã€GitHubèªè¨¼URLã‚’è¡¨ç¤ºã™ã‚‹\"\"\"\n",
        "#     import os\n",
        "\n",
        "#     # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã®ã¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
        "#     if not os.path.exists(\"code\"):\n",
        "#         print(\"â³ VS Code CLI ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...\")\n",
        "#         !curl -Lk 'https://code.visualstudio.com/sha/download?build=stable&os=cli-alpine-x64' --output vscode_cli.tar.gz\n",
        "#         !tar -xf vscode_cli.tar.gz\n",
        "\n",
        "#     print(\"\\n\" + \"=\"*50)\n",
        "#     print(\"ğŸš€ VS Code Tunnel ã‚’é–‹å§‹ã—ã¾ã™...\")\n",
        "#     print(\"â€» URLãŒè¡¨ç¤ºã•ã‚Œã‚‹ã¾ã§ 30ç§’ã€œ1åˆ†ã»ã©ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚\")\n",
        "#     print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "#     # èªè¨¼æƒ…å ±ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦ç¢ºå®Ÿã«æ–°ã—ã„ãƒªãƒ³ã‚¯ã‚’å‡ºã™\n",
        "#     # ä»¥å‰ã® `--provider` ã‚¨ãƒ©ãƒ¼ã‚’é¿ã‘ã‚‹ãŸã‚ã€æœ€å°é™ã®å¼•æ•°ã§å®Ÿè¡Œã—ã¾ã™\n",
        "#     !./code tunnel user logout\n",
        "#     !./code tunnel --accept-server-license-terms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_htIRXzRGC7R",
      "metadata": {
        "id": "_htIRXzRGC7R"
      },
      "outputs": [],
      "source": [
        "# !pkill code  # æ®‹ã£ã¦ã„ã‚‹VS Codeãƒ—ãƒ­ã‚»ã‚¹ã‚’å¼·åˆ¶çµ‚äº†"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
