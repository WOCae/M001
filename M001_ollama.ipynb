{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "83211fab",
      "metadata": {
        "id": "83211fab"
      },
      "source": [
        "ollamaã®èµ·å‹•"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f1f81f4",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q pypdf\n",
        "!pip install -q pypdf nbformat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43b7099a",
      "metadata": {
        "id": "43b7099a"
      },
      "outputs": [],
      "source": [
        "# 1. ä¾å­˜ãƒ„ãƒ¼ãƒ«ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "!apt-get update && apt-get install -y zstd pciutils\n",
        "\n",
        "# 2. Ollamaã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "\n",
        "# å…±é€šè¨­å®šï¼ˆã“ã“ã§ãƒ¢ãƒ‡ãƒ«åã‚’ä¸€æ‹¬ç®¡ç†ã—ã¾ã™ï¼‰\n",
        "# ä½¿ç”¨ã—ãŸã„ãƒ¢ãƒ‡ãƒ«åï¼ˆglm4, llama3, qwen2-vl ãªã©ï¼‰ã‚’ã“ã“ã«è¨˜è¿°\n",
        "MODEL_NAME = \"qwen3-vl:4b\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6c8f6c1",
      "metadata": {
        "id": "c6c8f6c1"
      },
      "outputs": [],
      "source": [
        "def setup_ollama_server(model_name):\n",
        "    \"\"\"ã‚µãƒ¼ãƒãƒ¼ã‚’å†èµ·å‹•ã—ã€æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’æº–å‚™ã™ã‚‹\"\"\"\n",
        "    print(\"ğŸ”„ ã‚µãƒ¼ãƒãƒ¼ã‚’ãƒªã‚»ãƒƒãƒˆä¸­...\")\n",
        "    !pkill ollama\n",
        "    time.sleep(2)\n",
        "\n",
        "    # ã‚µãƒ¼ãƒãƒ¼ã‚’ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§èµ·å‹•\n",
        "    subprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "    time.sleep(5)\n",
        "\n",
        "    print(f\"â³ ãƒ¢ãƒ‡ãƒ« '{model_name}' ã‚’æº–å‚™ä¸­...\")\n",
        "    subprocess.run([\"ollama\", \"pull\", model_name])\n",
        "\n",
        "    # æ¥ç¶šç¢ºèª\n",
        "    try:\n",
        "        res = requests.get(\"http://localhost:11434/api/tags\")\n",
        "        if res.status_code == 200:\n",
        "            models = [m['name'] for m in res.json()['models']]\n",
        "            print(f\"âœ… Ollama server is live! åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«: {models}\")\n",
        "            return True\n",
        "    except:\n",
        "        print(\"âŒ ã‚µãƒ¼ãƒãƒ¼ã®èµ·å‹•ã«å¤±æ•—ã—ã¾ã—ãŸã€‚\")\n",
        "        return False\n",
        "\n",
        "# å®Ÿè¡Œ\n",
        "setup_ollama_server(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fhni7ivwGNsf",
      "metadata": {
        "id": "fhni7ivwGNsf"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import requests\n",
        "import base64\n",
        "import nbformat\n",
        "from pypdf import PdfReader\n",
        "\n",
        "def extract_text_from_file(file_path):\n",
        "    ext = file_path.lower()\n",
        "    try:\n",
        "        if ext.endswith('.ipynb'):\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                nb = nbformat.read(f, as_version=4)\n",
        "                return \"\\n\".join([f\"[{c.cell_type}]\\n{c.source}\" for c in nb.cells if c.cell_type in ['code', 'markdown']])\n",
        "        elif ext.endswith('.pdf'):\n",
        "            reader = PdfReader(file_path)\n",
        "            return \"\\n\".join([p.extract_text() for p in reader.pages if p.extract_text()])\n",
        "        # ãƒ†ã‚­ã‚¹ãƒˆç³»ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆHTML, JS, CSS, Py, etc.ï¼‰\n",
        "        elif ext.endswith(('.txt', '.py', '.md', '.html', '.js', '.css', '.json', '.ts')):\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                return f.read()\n",
        "    except Exception as e:\n",
        "        return f\"èª­è¾¼ã‚¨ãƒ©ãƒ¼ ({file_path}): {str(e)}\"\n",
        "    return None\n",
        "\n",
        "def chat_with_model(message, history):\n",
        "    user_text = message[\"text\"]\n",
        "    user_files = message[\"files\"]\n",
        "    \n",
        "    model = MODEL_NAME if 'MODEL_NAME' in globals() else \"qwen3-vl:4b\"\n",
        "    url = \"http://localhost:11434/api/chat\"\n",
        "\n",
        "    file_contents = []\n",
        "    images_base64 = []\n",
        "\n",
        "    # è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒ£ãƒ³\n",
        "    for f in user_files:\n",
        "        if f.lower().endswith(('.png', '.jpg', '.jpeg', '.webp')):\n",
        "            images_base64.append(encode_image_to_base64(f))\n",
        "        else:\n",
        "            content = extract_text_from_file(f)\n",
        "            if content:\n",
        "                # ã©ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‹æ˜ç¢ºã«ã™ã‚‹ãŸã‚ã®è¦‹å‡ºã—ã‚’ä»˜ã‘ã‚‹\n",
        "                file_contents.append(f\"ãƒ•ã‚¡ã‚¤ãƒ«å: {f}\\nå†…å®¹:\\n{content}\\n\" + \"=\"*30)\n",
        "\n",
        "    # çµ±åˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä½œæˆ\n",
        "    if file_contents:\n",
        "        all_files_text = \"\\n\\n\".join(file_contents)\n",
        "        full_prompt = (\n",
        "            f\"ä»¥ä¸‹ã®é–¢é€£ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ç¾¤ã‚’è§£æã—ã¦ãã ã•ã„:\\n\\n{all_files_text}\\n\\n\"\n",
        "            f\"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä¾é ¼: {user_text}\"\n",
        "        )\n",
        "    else:\n",
        "        full_prompt = user_text\n",
        "\n",
        "    current_msg = {\"role\": \"user\", \"content\": full_prompt}\n",
        "    if images_base64:\n",
        "        current_msg[\"images\"] = images_base64\n",
        "\n",
        "    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã®æ§‹ç¯‰\n",
        "    msgs = []\n",
        "    for user_h, assistant_h in history:\n",
        "        msgs.append({\"role\": \"user\", \"content\": user_h})\n",
        "        msgs.append({\"role\": \"assistant\", \"content\": assistant_h})\n",
        "    msgs.append(current_msg)\n",
        "\n",
        "    try:\n",
        "        response = requests.post(url, json={\"model\": model, \"messages\": msgs, \"stream\": False}, timeout=180)\n",
        "        return response.json()['message']['content']\n",
        "    except Exception as e:\n",
        "        return f\"âš ï¸ ã‚¨ãƒ©ãƒ¼: {str(e)}\"\n",
        "\n",
        "# UIèµ·å‹• (è¤‡æ•°é¸æŠã‚’è¨±å¯)\n",
        "demo = gr.ChatInterface(\n",
        "    fn=chat_with_model,\n",
        "    title=\"ğŸ“‚ Multi-File & Full-Stack AI Explorer\",\n",
        "    multimodal=True,\n",
        "    textbox=gr.MultimodalTextbox(\n",
        "        file_types=[\"image\", \".txt\", \".pdf\", \".ipynb\", \".py\", \".html\", \".js\", \".css\", \".ts\"],\n",
        "        placeholder=\"ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¤‡æ•°é¸æŠï¼ˆãƒ‰ãƒ©ãƒƒã‚°ï¼†ãƒ‰ãƒ­ãƒƒãƒ—ï¼‰ã—ã¦ã€è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„...\"\n",
        "    ),\n",
        "    theme=\"soft\"\n",
        ")\n",
        "\n",
        "demo.launch(share=True, debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40a303d6",
      "metadata": {
        "id": "40a303d6"
      },
      "outputs": [],
      "source": [
        "# def launch_vscode_tunnel():\n",
        "#     \"\"\"VS Code Tunnelã‚’èµ·å‹•ã—ã€GitHubèªè¨¼URLã‚’è¡¨ç¤ºã™ã‚‹\"\"\"\n",
        "#     import os\n",
        "\n",
        "#     # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã®ã¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
        "#     if not os.path.exists(\"code\"):\n",
        "#         print(\"â³ VS Code CLI ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...\")\n",
        "#         !curl -Lk 'https://code.visualstudio.com/sha/download?build=stable&os=cli-alpine-x64' --output vscode_cli.tar.gz\n",
        "#         !tar -xf vscode_cli.tar.gz\n",
        "\n",
        "#     print(\"\\n\" + \"=\"*50)\n",
        "#     print(\"ğŸš€ VS Code Tunnel ã‚’é–‹å§‹ã—ã¾ã™...\")\n",
        "#     print(\"â€» URLãŒè¡¨ç¤ºã•ã‚Œã‚‹ã¾ã§ 30ç§’ã€œ1åˆ†ã»ã©ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚\")\n",
        "#     print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "#     # èªè¨¼æƒ…å ±ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦ç¢ºå®Ÿã«æ–°ã—ã„ãƒªãƒ³ã‚¯ã‚’å‡ºã™\n",
        "#     # ä»¥å‰ã® `--provider` ã‚¨ãƒ©ãƒ¼ã‚’é¿ã‘ã‚‹ãŸã‚ã€æœ€å°é™ã®å¼•æ•°ã§å®Ÿè¡Œã—ã¾ã™\n",
        "#     !./code tunnel user logout\n",
        "#     !./code tunnel --accept-server-license-terms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_htIRXzRGC7R",
      "metadata": {
        "id": "_htIRXzRGC7R"
      },
      "outputs": [],
      "source": [
        "# !pkill code  # æ®‹ã£ã¦ã„ã‚‹VS Codeãƒ—ãƒ­ã‚»ã‚¹ã‚’å¼·åˆ¶çµ‚äº†"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.10.11)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
