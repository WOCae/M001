{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "83211fab",
      "metadata": {
        "id": "83211fab"
      },
      "source": [
        "ollamaã®èµ·å‹•"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f1f81f4",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43b7099a",
      "metadata": {
        "id": "43b7099a"
      },
      "outputs": [],
      "source": [
        "# 1. ä¾å­˜ãƒ„ãƒ¼ãƒ«ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "!apt-get update && apt-get install -y zstd pciutils\n",
        "\n",
        "# 2. Ollamaã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "\n",
        "# å…±é€šè¨­å®šï¼ˆã“ã“ã§ãƒ¢ãƒ‡ãƒ«åã‚’ä¸€æ‹¬ç®¡ç†ã—ã¾ã™ï¼‰\n",
        "# ä½¿ç”¨ã—ãŸã„ãƒ¢ãƒ‡ãƒ«åï¼ˆglm4, llama3, qwen2-vl ãªã©ï¼‰ã‚’ã“ã“ã«è¨˜è¿°\n",
        "MODEL_NAME = \"qwen3-vl:4b\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6c8f6c1",
      "metadata": {
        "id": "c6c8f6c1"
      },
      "outputs": [],
      "source": [
        "def setup_ollama_server(model_name):\n",
        "    \"\"\"ã‚µãƒ¼ãƒãƒ¼ã‚’å†èµ·å‹•ã—ã€æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’æº–å‚™ã™ã‚‹\"\"\"\n",
        "    print(\"ğŸ”„ ã‚µãƒ¼ãƒãƒ¼ã‚’ãƒªã‚»ãƒƒãƒˆä¸­...\")\n",
        "    !pkill ollama\n",
        "    time.sleep(2)\n",
        "\n",
        "    # ã‚µãƒ¼ãƒãƒ¼ã‚’ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§èµ·å‹•\n",
        "    subprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "    time.sleep(5)\n",
        "\n",
        "    print(f\"â³ ãƒ¢ãƒ‡ãƒ« '{model_name}' ã‚’æº–å‚™ä¸­...\")\n",
        "    subprocess.run([\"ollama\", \"pull\", model_name])\n",
        "\n",
        "    # æ¥ç¶šç¢ºèª\n",
        "    try:\n",
        "        res = requests.get(\"http://localhost:11434/api/tags\")\n",
        "        if res.status_code == 200:\n",
        "            models = [m['name'] for m in res.json()['models']]\n",
        "            print(f\"âœ… Ollama server is live! åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«: {models}\")\n",
        "            return True\n",
        "    except:\n",
        "        print(\"âŒ ã‚µãƒ¼ãƒãƒ¼ã®èµ·å‹•ã«å¤±æ•—ã—ã¾ã—ãŸã€‚\")\n",
        "        return False\n",
        "\n",
        "# å®Ÿè¡Œ\n",
        "setup_ollama_server(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fhni7ivwGNsf",
      "metadata": {
        "id": "fhni7ivwGNsf"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import requests\n",
        "import json\n",
        "import base64\n",
        "from pypdf import PdfReader # PDFè§£æç”¨\n",
        "\n",
        "def extract_text_from_file(file_path):\n",
        "    \"\"\"ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹\"\"\"\n",
        "    if file_path.lower().endswith('.txt'):\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            return f.read()\n",
        "    elif file_path.lower().endswith('.pdf'):\n",
        "        reader = PdfReader(file_path)\n",
        "        text = \"\"\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text() + \"\\n\"\n",
        "        return text\n",
        "    return \"\"\n",
        "\n",
        "def encode_image_to_base64(image_path):\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        return base64.b64encode(image_file.read()).decode('utf-8')\n",
        "\n",
        "def chat_with_model(message, history):\n",
        "    user_text = message[\"text\"]\n",
        "    user_files = message[\"files\"]\n",
        "    \n",
        "    model = MODEL_NAME if 'MODEL_NAME' in globals() else \"qwen3-vl:4b\"\n",
        "    url = \"http://localhost:11434/api/chat\"\n",
        "\n",
        "    extracted_contexts = []\n",
        "    images_base64 = []\n",
        "\n",
        "    # ãƒ•ã‚¡ã‚¤ãƒ«ã®ä»•åˆ†ã‘ã¨å‡¦ç†\n",
        "    for f in user_files:\n",
        "        if f.lower().endswith(('.png', '.jpg', '.jpeg', '.webp')):\n",
        "            images_base64.append(encode_image_to_base64(f))\n",
        "        elif f.lower().endswith(('.txt', '.pdf')):\n",
        "            content = extract_text_from_file(f)\n",
        "            if content:\n",
        "                extracted_contexts.append(f\"--- File: {f} ---\\n{content}\")\n",
        "\n",
        "    # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«çµåˆ\n",
        "    full_prompt = user_text\n",
        "    if extracted_contexts:\n",
        "        context_str = \"\\n\\n\".join(extracted_contexts)\n",
        "        full_prompt = f\"ä»¥ä¸‹ã®è³‡æ–™ã‚’å‚è€ƒã«ã—ã¦å›ç­”ã—ã¦ãã ã•ã„:\\n\\n{context_str}\\n\\nè³ªå•: {user_text}\"\n",
        "\n",
        "    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ§‹é€ ã®ä½œæˆ\n",
        "    current_msg = {\"role\": \"user\", \"content\": full_prompt}\n",
        "    if images_base64:\n",
        "        current_msg[\"images\"] = images_base64\n",
        "\n",
        "    # å±¥æ­´ã®è¿½åŠ \n",
        "    msgs = []\n",
        "    for user_msg, ai_msg in history:\n",
        "        # å±¥æ­´ã¯ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’ä¿æŒï¼ˆã‚·ãƒ³ãƒ—ãƒ«åŒ–ã®ãŸã‚ï¼‰\n",
        "        msgs.append({\"role\": \"user\", \"content\": user_msg})\n",
        "        msgs.append({\"role\": \"assistant\", \"content\": ai_msg})\n",
        "    \n",
        "    msgs.append(current_msg)\n",
        "\n",
        "    payload = {\"model\": model, \"messages\": msgs, \"stream\": False}\n",
        "\n",
        "    try:\n",
        "        response = requests.post(url, json=payload, timeout=180) # é•·æ–‡å¯¾å¿œã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’å»¶é•·\n",
        "        response.raise_for_status()\n",
        "        return response.json()['message']['content']\n",
        "    except Exception as e:\n",
        "        return f\"âš ï¸ ã‚¨ãƒ©ãƒ¼: {str(e)}\"\n",
        "\n",
        "# UIã®è¨­å®š\n",
        "demo = gr.ChatInterface(\n",
        "    fn=chat_with_model,\n",
        "    title=f\"ğŸ¤– {MODEL_NAME} Advanced Chat UI\",\n",
        "    description=\"ç”»åƒã€ãƒ†ã‚­ã‚¹ãƒˆã€PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦è³ªå•ã§ãã¾ã™ã€‚\",\n",
        "    multimodal=True,\n",
        "    textbox=gr.MultimodalTextbox(file_types=[\"image\", \".txt\", \".pdf\"]),\n",
        "    theme=\"soft\"\n",
        ")\n",
        "\n",
        "demo.launch(share=True, debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40a303d6",
      "metadata": {
        "id": "40a303d6"
      },
      "outputs": [],
      "source": [
        "# def launch_vscode_tunnel():\n",
        "#     \"\"\"VS Code Tunnelã‚’èµ·å‹•ã—ã€GitHubèªè¨¼URLã‚’è¡¨ç¤ºã™ã‚‹\"\"\"\n",
        "#     import os\n",
        "\n",
        "#     # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã®ã¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰\n",
        "#     if not os.path.exists(\"code\"):\n",
        "#         print(\"â³ VS Code CLI ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...\")\n",
        "#         !curl -Lk 'https://code.visualstudio.com/sha/download?build=stable&os=cli-alpine-x64' --output vscode_cli.tar.gz\n",
        "#         !tar -xf vscode_cli.tar.gz\n",
        "\n",
        "#     print(\"\\n\" + \"=\"*50)\n",
        "#     print(\"ğŸš€ VS Code Tunnel ã‚’é–‹å§‹ã—ã¾ã™...\")\n",
        "#     print(\"â€» URLãŒè¡¨ç¤ºã•ã‚Œã‚‹ã¾ã§ 30ç§’ã€œ1åˆ†ã»ã©ã‹ã‹ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚\")\n",
        "#     print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "#     # èªè¨¼æƒ…å ±ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦ç¢ºå®Ÿã«æ–°ã—ã„ãƒªãƒ³ã‚¯ã‚’å‡ºã™\n",
        "#     # ä»¥å‰ã® `--provider` ã‚¨ãƒ©ãƒ¼ã‚’é¿ã‘ã‚‹ãŸã‚ã€æœ€å°é™ã®å¼•æ•°ã§å®Ÿè¡Œã—ã¾ã™\n",
        "#     !./code tunnel user logout\n",
        "#     !./code tunnel --accept-server-license-terms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_htIRXzRGC7R",
      "metadata": {
        "id": "_htIRXzRGC7R"
      },
      "outputs": [],
      "source": [
        "# !pkill code  # æ®‹ã£ã¦ã„ã‚‹VS Codeãƒ—ãƒ­ã‚»ã‚¹ã‚’å¼·åˆ¶çµ‚äº†"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.10.11)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
